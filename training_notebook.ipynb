{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Stage Medical LLM Fine-tuning (Llama 3 8B)\n",
                "\n",
                "This notebook implements a 4-stage training pipeline to create a high-quality medical chatbot.\n",
                "\n",
                "## Pipeline Stages\n",
                "1.  **Stage 1: Instruction Tuning** (Alpaca / General Instructions)\n",
                "    - Goal: Teach the model to follow instructions and format responses.\n",
                "2.  **Stage 2: Domain Adaptation** (HealthCareMagic, iCliniq, MedDialog, PDF)\n",
                "    - Goal: Infuse medical knowledge and reasoning capabilities.\n",
                "3.  **Stage 3: Medicine Recommendation** (MIMIC-IV, DrugBank)\n",
                "    - Goal: Learn safe medication recommendations and drug interactions.\n",
                "4.  **Stage 4: Follow-up Questions** (FollowupQ)\n",
                "    - Goal: Learn to ask relevant follow-up questions to clarify patient queries.\n",
                "\n",
                "## Requirements\n",
                "- Google Colab (T4 GPU or better)\n",
                "- Google Drive mounted with datasets in `doctor_online_data/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "import torch\n",
                "major_version, minor_version = torch.cuda.get_device_capability()\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "if major_version >= 8:\n",
                "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
                "else:\n",
                "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
                "!pip install PyPDF2 pandas datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "from datasets import Dataset, concatenate_datasets, load_dataset\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "import PyPDF2\n",
                "\n",
                "# Setup Paths (Robust to Local/Colab)\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    BASE_PATH = \"/content/drive/MyDrive/doctor_online_data/\"\n",
                "    OUTPUT_DIR = \"/content/drive/MyDrive/doctor_online_data/checkpoints/\"\n",
                "except Exception as e:\n",
                "    print(f\"Drive mount failed or not on Colab: {e}\")\n",
                "    print(\"Using local paths instead.\")\n",
                "    BASE_PATH = \"./doctor_online_data/\"\n",
                "    OUTPUT_DIR = \"./doctor_online_data/checkpoints/\"\n",
                "\n",
                "if not os.path.exists(BASE_PATH):\n",
                "    os.makedirs(BASE_PATH, exist_ok=True)\n",
                "    print(f\"Created local data directory at {BASE_PATH}. Please put your datasets here.\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Base Path: {BASE_PATH}\")\n",
                "print(f\"Output Dir: {OUTPUT_DIR}\")\n",
                "\n",
                "# Model Config\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                ")\n",
                "\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "{}\n",
                "\n",
                "### Input:\n",
                "{}\n",
                "\n",
                "### Response:\n",
                "{}\"\"\"\n",
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    instructions = examples[\"instruction\"]\n",
                "    inputs       = examples[\"input\"]\n",
                "    outputs      = examples[\"output\"]\n",
                "    texts = []\n",
                "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
                "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Helper Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_stage(stage_name, dataset, epochs=1, max_steps=-1):\n",
                "    print(f\"\\n=== Starting {stage_name} ===\")\n",
                "    print(f\"Dataset size: {len(dataset)}\")\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model = model,\n",
                "        tokenizer = tokenizer,\n",
                "        train_dataset = dataset,\n",
                "        dataset_text_field = \"text\",\n",
                "        max_seq_length = max_seq_length,\n",
                "        dataset_num_proc = 2,\n",
                "        packing = False,\n",
                "        args = TrainingArguments(\n",
                "            per_device_train_batch_size = 2,\n",
                "            gradient_accumulation_steps = 4,\n",
                "            warmup_steps = 5,\n",
                "            num_train_epochs = epochs,\n",
                "            max_steps = max_steps,\n",
                "            learning_rate = 2e-4,\n",
                "            fp16 = not torch.cuda.is_bf16_supported(),\n",
                "            bf16 = torch.cuda.is_bf16_supported(),\n",
                "            logging_steps = 10,\n",
                "            optim = \"adamw_8bit\",\n",
                "            weight_decay = 0.01,\n",
                "            lr_scheduler_type = \"linear\",\n",
                "            seed = 3407,\n",
                "            output_dir = f\"{OUTPUT_DIR}/{stage_name}\",\n",
                "        ),\n",
                "    )\n",
                "    trainer.train()\n",
                "    # Save adapter for this stage\n",
                "    model.save_pretrained(f\"{OUTPUT_DIR}/{stage_name}_adapter\")\n",
                "    print(f\"=== Completed {stage_name} ===\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 1: Instruction Tuning (Alpaca)\n",
                "We use the standard Alpaca dataset or your `cleaned_dataset` to establish basic instruction following."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try to load local cleaned dataset, else fallback to HF Alpaca\n",
                "ds_stage1 = None\n",
                "local_path = f\"{BASE_PATH}cleaned_dataset_with_english_translation.csv\"\n",
                "\n",
                "if os.path.exists(local_path):\n",
                "    print(\"Loading local Stage 1 dataset...\")\n",
                "    df = pd.read_csv(local_path)\n",
                "    # Normalize columns\n",
                "    if 'instruction' not in df.columns: df['instruction'] = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
                "    if 'input' not in df.columns: df['input'] = df.get('description', '')\n",
                "    if 'output' not in df.columns: df['output'] = df.get('doctor_response', '')\n",
                "    ds_stage1 = Dataset.from_pandas(df[['instruction', 'input', 'output']])\n",
                "else:\n",
                "    print(\"Local dataset not found. Loading yahma/alpaca-cleaned from Hugging Face...\")\n",
                "    ds_stage1 = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
                "\n",
                "ds_stage1 = ds_stage1.map(formatting_prompts_func, batched = True)\n",
                "train_stage(\"Stage1_Instruction\", ds_stage1, max_steps=100) # Short run for demo, increase steps for real training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 2: Domain Adaptation (Medical)\n",
                "Training on `HealthCareMagic`, `iCliniq`, and your PDF to learn medical reasoning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets_stage2 = []\n",
                "\n",
                "# HealthCareMagic\n",
                "hcm_path = f\"{BASE_PATH}HealthCareMagic-100k.json\"\n",
                "if os.path.exists(hcm_path):\n",
                "    with open(hcm_path, 'r') as f: data = json.load(f)\n",
                "    datasets_stage2.append(Dataset.from_list(data))\n",
                "    print(f\"Loaded HealthCareMagic: {len(data)} rows\")\n",
                "\n",
                "# iCliniq\n",
                "icliniq_path = f\"{BASE_PATH}iCliniq.json\"\n",
                "if os.path.exists(icliniq_path):\n",
                "    with open(icliniq_path, 'r') as f: data = json.load(f)\n",
                "    formatted = []\n",
                "    for item in data:\n",
                "        ans = item.get('answer_chatdoctor') or item.get('answer_icliniq')\n",
                "        if ans:\n",
                "            formatted.append({\n",
                "                \"instruction\": \"If you are a doctor, please answer the medical questions based on the patient's description.\",\n",
                "                \"input\": item.get('input', ''),\n",
                "                \"output\": ans\n",
                "            })\n",
                "    datasets_stage2.append(Dataset.from_list(formatted))\n",
                "    print(f\"Loaded iCliniq: {len(formatted)} rows\")\n",
                "\n",
                "# PDF (2503.17509v1.pdf)\n",
                "pdf_path = f\"{BASE_PATH}2503.17509v1.pdf\"\n",
                "if os.path.exists(pdf_path):\n",
                "    pdf_text = \"\"\n",
                "    try:\n",
                "        with open(pdf_path, 'rb') as f:\n",
                "            reader = PyPDF2.PdfReader(f)\n",
                "            for page in reader.pages:\n",
                "                pdf_text += page.extract_text() + \"\\n\"\n",
                "        \n",
                "        # Chunk text\n",
                "        chunk_size = 1000\n",
                "        chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
                "        \n",
                "        pdf_data = []\n",
                "        for chunk in chunks:\n",
                "            pdf_data.append({\n",
                "                \"instruction\": \"Analyze this medical text and summarize key findings.\",\n",
                "                \"input\": chunk,\n",
                "                \"output\": \"The text discusses medical concepts found in the document. (Self-supervised context)\"\n",
                "            })\n",
                "        datasets_stage2.append(Dataset.from_list(pdf_data))\n",
                "        print(f\"Loaded PDF: {len(pdf_data)} chunks\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading PDF: {e}\")\n",
                "\n",
                "if datasets_stage2:\n",
                "    ds_stage2 = concatenate_datasets(datasets_stage2)\n",
                "    ds_stage2 = ds_stage2.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage2_Domain\", ds_stage2, max_steps=200)\n",
                "else:\n",
                "    print(\"Skipping Stage 2: No datasets found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 3: Medicine Recommendation (Safety)\n",
                "Training on MIMIC-IV / DrugBank for safe prescribing. \n",
                "**Note**: Requires `mimic_iv.csv` or `drugbank.json` in your Drive folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_stage3 = None\n",
                "mimic_path = f\"{BASE_PATH}mimic_iv.csv\"\n",
                "drugbank_path = f\"{BASE_PATH}drugbank.json\"\n",
                "\n",
                "if os.path.exists(mimic_path):\n",
                "    print(\"Loading MIMIC-IV...\")\n",
                "    df = pd.read_csv(mimic_path)\n",
                "    # Expects columns: patient_profile, medication_plan\n",
                "    if 'patient_profile' in df.columns and 'medication_plan' in df.columns:\n",
                "        df['instruction'] = \"Based on the patient profile, recommend a safe medication plan.\"\n",
                "        df['input'] = df['patient_profile']\n",
                "        df['output'] = df['medication_plan']\n",
                "        ds_stage3 = Dataset.from_pandas(df[['instruction', 'input', 'output']])\n",
                "\n",
                "if ds_stage3:\n",
                "    ds_stage3 = ds_stage3.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage3_Meds\", ds_stage3, max_steps=100)\n",
                "else:\n",
                "    print(\"Skipping Stage 3: MIMIC/DrugBank data not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 4: Follow-up Questions\n",
                "Training the model to ask clarifying questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_stage4 = None\n",
                "followup_path = f\"{BASE_PATH}followup_q.json\"\n",
                "\n",
                "if os.path.exists(followup_path):\n",
                "    print(\"Loading FollowupQ...\")\n",
                "    with open(followup_path, 'r') as f: data = json.load(f)\n",
                "    # Expects: context, answer, followup\n",
                "    formatted = []\n",
                "    for item in data:\n",
                "        formatted.append({\n",
                "            \"instruction\": \"Given the patient context and doctor answer, generate a relevant follow-up question.\",\n",
                "            \"input\": f\"Context: {item.get('context')}\\nAnswer: {item.get('answer')}\",\n",
                "            \"output\": item.get('followup')\n",
                "        })\n",
                "    ds_stage4 = Dataset.from_list(formatted)\n",
                "\n",
                "if ds_stage4:\n",
                "    ds_stage4 = ds_stage4.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage4_Followup\", ds_stage4, max_steps=50)\n",
                "else:\n",
                "    print(\"Skipping Stage 4: FollowupQ data not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Export Final Model\n",
                "We free up memory before exporting to avoid crashes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gc\n",
                "# Free up memory from training\n",
                "if 'trainer' in globals():\n",
                "    del trainer\n",
                "if 'ds_stage1' in globals(): del ds_stage1\n",
                "if 'ds_stage2' in globals(): del ds_stage2\n",
                "if 'ds_stage3' in globals(): del ds_stage3\n",
                "if 'ds_stage4' in globals(): del ds_stage4\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final merged model to GGUF\n",
                "if 'model' not in globals():\n",
                "    print(\"Error: 'model' variable not found. Please run the training cells first.\")\n",
                "else:\n",
                "    print(\"Saving final model to GGUF format...\")\n",
                "    # Save to q4_k_m (balanced) and q8_0 (high quality)\n",
                "    \n",
                "    # 1. Save Q4_K_M (Recommended for most users)\n",
                "    try:\n",
                "        model.save_pretrained_gguf(\"model_q4\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "        !cp model_q4-unsloth.Q4_K_M.gguf {OUTPUT_DIR}/medical_llama3_q4_k_m.gguf\n",
                "        print(f\"Saved Q4_K_M to {OUTPUT_DIR}/medical_llama3_q4_k_m.gguf\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error saving Q4_K_M: {e}\")\n",
                "\n",
                "    # 2. Save Q8_0 (High precision)\n",
                "    # try:\n",
                "    #     model.save_pretrained_gguf(\"model_q8\", tokenizer, quantization_method = \"q8_0\")\n",
                "    #     !cp model_q8-unsloth.Q8_0.gguf {OUTPUT_DIR}/medical_llama3_q8_0.gguf\n",
                "    #     print(f\"Saved Q8_0 to {OUTPUT_DIR}/medical_llama3_q8_0.gguf\")\n",
                "    # except Exception as e:\n",
                "    #     print(f\"Error saving Q8_0: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}