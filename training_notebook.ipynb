{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Stage Medical LLM Fine-tuning (Llama 3 8B)\n",
                "\n",
                "This notebook implements a 4-stage training pipeline to create a high-quality medical chatbot.\n",
                "\n",
                "## Pipeline Stages\n",
                "1.  **Stage 1: Instruction Tuning** (Alpaca / General Instructions)\n",
                "    - Goal: Teach the model to follow instructions and format responses.\n",
                "2.  **Stage 2: Domain Adaptation** (HealthCareMagic, iCliniq, MedDialog, PDF)\n",
                "    - Goal: Infuse medical knowledge and reasoning capabilities.\n",
                "3.  **Stage 3: Medicine Recommendation** (MIMIC-IV, DrugBank)\n",
                "    - Goal: Learn safe medication recommendations and drug interactions.\n",
                "4.  **Stage 4: Follow-up Questions** (FollowupQ)\n",
                "    - Goal: Learn to ask relevant follow-up questions to clarify patient queries.\n",
                "\n",
                "## Requirements\n",
                "- Google Colab (T4 GPU or better)\n",
                "- Google Drive mounted with datasets in `doctor_online_data/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "import torch\n",
                "major_version, minor_version = torch.cuda.get_device_capability()\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "if major_version >= 8:\n",
                "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
                "else:\n",
                "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
                "!pip install PyPDF2 pandas datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
                        "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
                        "Drive mount failed or not on Colab: mount failed\n",
                        "Using local paths instead.\n",
                        "Base Path: ./doctor_online_data/\n",
                        "Output Dir: ./doctor_online_data/checkpoints/\n",
                        "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.1.\n",
                        "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
                        "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
                        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
                        " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
                        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
                    ]
                }
            ],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "from datasets import Dataset, concatenate_datasets, load_dataset\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "import PyPDF2\n",
                "\n",
                "# Setup Paths (Robust to Local/Colab)\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    BASE_PATH = \"/content/drive/MyDrive/doctor_online_data/\"\n",
                "    OUTPUT_DIR = \"/content/drive/MyDrive/doctor_online_data/checkpoints/\"\n",
                "except Exception as e:\n",
                "    print(f\"Drive mount failed or not on Colab: {e}\")\n",
                "    print(\"Using local paths instead.\")\n",
                "    BASE_PATH = \"./doctor_online_data/\"\n",
                "    OUTPUT_DIR = \"./doctor_online_data/checkpoints/\"\n",
                "\n",
                "if not os.path.exists(BASE_PATH):\n",
                "    os.makedirs(BASE_PATH, exist_ok=True)\n",
                "    print(f\"Created local data directory at {BASE_PATH}. Please put your datasets here.\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Base Path: {BASE_PATH}\")\n",
                "print(f\"Output Dir: {OUTPUT_DIR}\")\n",
                "\n",
                "# Model Config - LOW MEMORY SETTINGS\n",
                "max_seq_length = 1024 # Reduced from 2048 to save memory\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                ")\n",
                "\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "{}\n",
                "\n",
                "### Input:\n",
                "{}\n",
                "\n",
                "### Response:\n",
                "{}\"\"\"\n",
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    instructions = examples[\"instruction\"]\n",
                "    inputs       = examples[\"input\"]\n",
                "    outputs      = examples[\"output\"]\n",
                "    texts = []\n",
                "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
                "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Helper Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_stage(stage_name, dataset, epochs=1, max_steps=-1):\n",
                "    print(f\"\\n=== Starting {stage_name} ===\")\n",
                "    print(f\"Dataset size: {len(dataset)}\")\n",
                "    \n",
                "    # Clear cache before training\n",
                "    torch.cuda.empty_cache()\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model = model,\n",
                "        tokenizer = tokenizer,\n",
                "        train_dataset = dataset,\n",
                "        dataset_text_field = \"text\",\n",
                "        max_seq_length = max_seq_length,\n",
                "        dataset_num_proc = 2,\n",
                "        packing = False,\n",
                "        args = TrainingArguments(\n",
                "            per_device_train_batch_size = 1, # Reduced to 1\n",
                "            gradient_accumulation_steps = 8, # Increased to 8 to maintain effective batch size\n",
                "            warmup_steps = 5,\n",
                "            num_train_epochs = epochs,\n",
                "            max_steps = max_steps,\n",
                "            learning_rate = 2e-4,\n",
                "            fp16 = not torch.cuda.is_bf16_supported(),\n",
                "            bf16 = torch.cuda.is_bf16_supported(),\n",
                "            logging_steps = 10,\n",
                "            optim = \"adamw_8bit\",\n",
                "            weight_decay = 0.01,\n",
                "            lr_scheduler_type = \"linear\",\n",
                "            seed = 3407,\n",
                "            output_dir = f\"{OUTPUT_DIR}/{stage_name}\",\n",
                "        ),\n",
                "    )\n",
                "    trainer.train()\n",
                "    # Save adapter for this stage\n",
                "    model.save_pretrained(f\"{OUTPUT_DIR}/{stage_name}_adapter\")\n",
                "    print(f\"=== Completed {stage_name} ===\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 1: Instruction Tuning (Alpaca)\n",
                "We use the standard Alpaca dataset or your `cleaned_dataset` to establish basic instruction following."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Local dataset not found. Loading yahma/alpaca-cleaned from Hugging Face...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0a8cc6e07d0d41f980152c9ce865bf79",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Starting Stage1_Instruction ===\n",
                        "Dataset size: 51760\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a9b16666087147058e6012718bbf0d7b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/51760 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
                        "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
                        "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 100\n",
                        "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
                        "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
                        " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n",
                        "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
                        "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaslari610\u001b[0m (\u001b[33manaslari610-bengal-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/content/wandb/run-20251126_180113-53gve2e8</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface/runs/53gve2e8' target=\"_blank\">polar-meadow-6</a></strong> to <a href='https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface/runs/53gve2e8' target=\"_blank\">https://wandb.ai/anaslari610-bengal-institute-of-technology/huggingface/runs/53gve2e8</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unsloth: Will smartly offload gradients to save VRAM!\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='78' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 78/100 10:23 < 03:00, 0.12 it/s, Epoch 0.01/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>1.495700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>1.002900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>0.972400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>0.931600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>0.959800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>60</td>\n",
                            "      <td>0.939800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>70</td>\n",
                            "      <td>0.961200</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Try to load local cleaned dataset, else fallback to HF Alpaca\n",
                "ds_stage1 = None\n",
                "local_path = f\"{BASE_PATH}cleaned_dataset_with_english_translation.csv\"\n",
                "\n",
                "if os.path.exists(local_path):\n",
                "    print(\"Loading local Stage 1 dataset...\")\n",
                "    df = pd.read_csv(local_path)\n",
                "    # Normalize columns\n",
                "    if 'instruction' not in df.columns: df['instruction'] = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
                "    if 'input' not in df.columns: df['input'] = df.get('description', '')\n",
                "    if 'output' not in df.columns: df['output'] = df.get('doctor_response', '')\n",
                "    ds_stage1 = Dataset.from_pandas(df[['instruction', 'input', 'output']])\n",
                "else:\n",
                "    print(\"Local dataset not found. Loading yahma/alpaca-cleaned from Hugging Face...\")\n",
                "    ds_stage1 = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
                "\n",
                "ds_stage1 = ds_stage1.map(formatting_prompts_func, batched = True)\n",
                "train_stage(\"Stage1_Instruction\", ds_stage1, max_steps=100) # Short run for demo, increase steps for real training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 2: Domain Adaptation (Medical)\n",
                "Training on `HealthCareMagic`, `iCliniq`, and your PDF to learn medical reasoning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets_stage2 = []\n",
                "\n",
                "# HealthCareMagic\n",
                "hcm_path = f\"{BASE_PATH}HealthCareMagic-100k.json\"\n",
                "if os.path.exists(hcm_path):\n",
                "    with open(hcm_path, 'r') as f: data = json.load(f)\n",
                "    datasets_stage2.append(Dataset.from_list(data))\n",
                "    print(f\"Loaded HealthCareMagic: {len(data)} rows\")\n",
                "\n",
                "# iCliniq\n",
                "icliniq_path = f\"{BASE_PATH}iCliniq.json\"\n",
                "if os.path.exists(icliniq_path):\n",
                "    with open(icliniq_path, 'r') as f: data = json.load(f)\n",
                "    formatted = []\n",
                "    for item in data:\n",
                "        ans = item.get('answer_chatdoctor') or item.get('answer_icliniq')\n",
                "        if ans:\n",
                "            formatted.append({\n",
                "                \"instruction\": \"If you are a doctor, please answer the medical questions based on the patient's description.\",\n",
                "                \"input\": item.get('input', ''),\n",
                "                \"output\": ans\n",
                "            })\n",
                "    datasets_stage2.append(Dataset.from_list(formatted))\n",
                "    print(f\"Loaded iCliniq: {len(formatted)} rows\")\n",
                "\n",
                "# PDF (2503.17509v1.pdf)\n",
                "pdf_path = f\"{BASE_PATH}2503.17509v1.pdf\"\n",
                "if os.path.exists(pdf_path):\n",
                "    pdf_text = \"\"\n",
                "    try:\n",
                "        with open(pdf_path, 'rb') as f:\n",
                "            reader = PyPDF2.PdfReader(f)\n",
                "            for page in reader.pages:\n",
                "                pdf_text += page.extract_text() + \"\\n\"\n",
                "        \n",
                "        # Chunk text\n",
                "        chunk_size = 1000\n",
                "        chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
                "        \n",
                "        pdf_data = []\n",
                "        for chunk in chunks:\n",
                "            pdf_data.append({\n",
                "                \"instruction\": \"Analyze this medical text and summarize key findings.\",\n",
                "                \"input\": chunk,\n",
                "                \"output\": \"The text discusses medical concepts found in the document. (Self-supervised context)\"\n",
                "            })\n",
                "        datasets_stage2.append(Dataset.from_list(pdf_data))\n",
                "        print(f\"Loaded PDF: {len(pdf_data)} chunks\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading PDF: {e}\")\n",
                "\n",
                "if datasets_stage2:\n",
                "    ds_stage2 = concatenate_datasets(datasets_stage2)\n",
                "    ds_stage2 = ds_stage2.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage2_Domain\", ds_stage2, max_steps=200)\n",
                "else:\n",
                "    print(\"Skipping Stage 2: No datasets found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 3: Medicine Recommendation (Safety)\n",
                "Training on MIMIC-IV / DrugBank for safe prescribing. \n",
                "**Note**: Requires `mimic_iv.csv` or `drugbank.json` in your Drive folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_stage3 = None\n",
                "mimic_path = f\"{BASE_PATH}mimic_iv.csv\"\n",
                "drugbank_path = f\"{BASE_PATH}drugbank.json\"\n",
                "\n",
                "if os.path.exists(mimic_path):\n",
                "    print(\"Loading MIMIC-IV...\")\n",
                "    df = pd.read_csv(mimic_path)\n",
                "    # Expects columns: patient_profile, medication_plan\n",
                "    if 'patient_profile' in df.columns and 'medication_plan' in df.columns:\n",
                "        df['instruction'] = \"Based on the patient profile, recommend a safe medication plan.\"\n",
                "        df['input'] = df['patient_profile']\n",
                "        df['output'] = df['medication_plan']\n",
                "        ds_stage3 = Dataset.from_pandas(df[['instruction', 'input', 'output']])\n",
                "\n",
                "if ds_stage3:\n",
                "    ds_stage3 = ds_stage3.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage3_Meds\", ds_stage3, max_steps=100)\n",
                "else:\n",
                "    print(\"Skipping Stage 3: MIMIC/DrugBank data not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 4: Follow-up Questions\n",
                "Training the model to ask clarifying questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_stage4 = None\n",
                "followup_path = f\"{BASE_PATH}followup_q.json\"\n",
                "\n",
                "if os.path.exists(followup_path):\n",
                "    print(\"Loading FollowupQ...\")\n",
                "    with open(followup_path, 'r') as f: data = json.load(f)\n",
                "    # Expects: context, answer, followup\n",
                "    formatted = []\n",
                "    for item in data:\n",
                "        formatted.append({\n",
                "            \"instruction\": \"Given the patient context and doctor answer, generate a relevant follow-up question.\",\n",
                "            \"input\": f\"Context: {item.get('context')}\\nAnswer: {item.get('answer')}\",\n",
                "            \"output\": item.get('followup')\n",
                "        })\n",
                "    ds_stage4 = Dataset.from_list(formatted)\n",
                "\n",
                "if ds_stage4:\n",
                "    ds_stage4 = ds_stage4.map(formatting_prompts_func, batched = True)\n",
                "    train_stage(\"Stage4_Followup\", ds_stage4, max_steps=50)\n",
                "else:\n",
                "    print(\"Skipping Stage 4: FollowupQ data not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Export Final Model\n",
                "We free up memory before exporting to avoid crashes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gc\n",
                "# Free up memory from training\n",
                "if 'trainer' in globals():\n",
                "    del trainer\n",
                "if 'ds_stage1' in globals(): del ds_stage1\n",
                "if 'ds_stage2' in globals(): del ds_stage2\n",
                "if 'ds_stage3' in globals(): del ds_stage3\n",
                "if 'ds_stage4' in globals(): del ds_stage4\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final merged model to GGUF\n",
                "if 'model' not in globals():\n",
                "    print(\"Error: 'model' variable not found. Please run the training cells first.\")\n",
                "else:\n",
                "    print(\"Saving final model to GGUF format...\")\n",
                "    # Save to q4_k_m (balanced) and q8_0 (high quality)\n",
                "    \n",
                "    # 1. Save Q4_K_M (Recommended for most users)\n",
                "    try:\n",
                "        model.save_pretrained_gguf(\"model_q4\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "        !cp model_q4-unsloth.Q4_K_M.gguf {OUTPUT_DIR}/medical_llama3_q4_k_m.gguf\n",
                "        print(f\"Saved Q4_K_M to {OUTPUT_DIR}/medical_llama3_q4_k_m.gguf\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error saving Q4_K_M: {e}\")\n",
                "\n",
                "    # 2. Save Q8_0 (High precision)\n",
                "    # try:\n",
                "    #     model.save_pretrained_gguf(\"model_q8\", tokenizer, quantization_method = \"q8_0\")\n",
                "    #     !cp model_q8-unsloth.Q8_0.gguf {OUTPUT_DIR}/medical_llama3_q8_0.gguf\n",
                "    #     print(f\"Saved Q8_0 to {OUTPUT_DIR}/medical_llama3_q8_0.gguf\")\n",
                "    # except Exception as e:\n",
                "    #     print(f\"Error saving Q8_0: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
