from __future__ import annotations

import argparse
import sys
from textwrap import dedent

from llama_cpp import Llama


def load_model(
    repo_id: str = "mradermacher/Gemma-2-2b-it-chat-MedQuad-GGUF",
    filename: str = "Gemma-2-2b-it-chat-MedQuad.IQ4_XS.gguf",
    n_ctx: int = 4096,
    n_threads: int | None = None,
) -> Llama:
    """Load the MedQuAD-tuned Gemma GGUF model via llama-cpp-python.

    The model weights will be downloaded from Hugging Face on first use.
    """

    kwargs: dict = {
        "repo_id": repo_id,
        "filename": filename,
        "n_ctx": n_ctx,
    }
    if n_threads is not None:
        kwargs["n_threads"] = n_threads

    return Llama.from_pretrained(**kwargs)


def med_reasoning(llm: Llama, symptoms: str) -> str:
    """Run symptoms → disease → treatment reasoning with safety framing."""

    messages = [
        {
            "role": "system",
            "content": (
                "You are a clinical reasoning assistant. "
                "Given patient symptoms, you:\n"
                "1) Summarize key findings.\n"
                "2) Provide a differential diagnosis with brief justification.\n"
                "3) Suggest initial investigations.\n"
                "4) Outline possible treatment options and red-flag signs.\n"
                "This is for educational use only and is not medical advice."
            ),
        },
        {
            "role": "user",
            "content": f"Symptoms: {symptoms}",
        },
    ]

    response = llm.create_chat_completion(
        messages=messages,
        temperature=0.4,
        max_tokens=512,
    )
    return response["choices"][0]["message"]["content"]


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Run MedQuAD Gemma model for symptoms → disease → treatment reasoning.",
    )
    parser.add_argument(
        "symptoms",
        nargs="?",
        help="Patient symptoms description. If omitted, will read from stdin.",
    )
    parser.add_argument(
        "--threads",
        type=int,
        default=None,
        help="Number of CPU threads to use for inference.",
    )

    args = parser.parse_args(argv)

    if args.symptoms:
        symptoms = args.symptoms
    else:
        print("Enter symptoms (end with Ctrl-D):", file=sys.stderr)
        symptoms = sys.stdin.read().strip()
        if not symptoms:
            print("No symptoms provided.", file=sys.stderr)
            return 1

    print("[Loading MedQuAD Gemma model… this may take a while on first run]", file=sys.stderr)
    llm = load_model(n_threads=args.threads)

    print("[Running clinical reasoning…]", file=sys.stderr)
    output = med_reasoning(llm, symptoms)

    print()
    print("=== Model response ===")
    print(output)
    print()
    print(
        dedent(
            """
            Disclaimer: This output is generated by an AI model and is provided for
            educational and research purposes only. It is not medical advice and
            must not be used for diagnosis or treatment decisions. Always consult
            a licensed healthcare professional.
            """
        ).strip()
    )

    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
