{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Medical Model Export (Kaggle Edition)\n",
                "\n",
                "This notebook exports your fine-tuned adapters to GGUF format using Kaggle's GPUs.\n",
                "\n",
                "## Prerequisites\n",
                "1.  **Upload Checkpoints**: You must upload your `checkpoints` folder as a Kaggle Dataset.\n",
                "2.  **Add Dataset**: Add that dataset to this notebook (it will appear in `/kaggle/input`).\n",
                "3.  **GPU**: Ensure Accelerator is set to GPU T4 x2 or P100."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "import torch\n",
                "major_version, minor_version = torch.cuda.get_device_capability()\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "if major_version >= 8:\n",
                "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
                "else:\n",
                "    !pip install --no-deps xformers trl peft accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "import os\n",
                "\n",
                "# Kaggle Input Directory\n",
                "INPUT_DIR = \"/kaggle/input/\"\n",
                "OUTPUT_DIR = \"/kaggle/working/\"\n",
                "\n",
                "print(f\"Listing all files in {INPUT_DIR} to find checkpoints...\")\n",
                "CHECKPOINT_DIR = None\n",
                "\n",
                "# Recursive search for 'adapter_config.json' to find the actual adapter folder\n",
                "adapter_candidates = []\n",
                "for root, dirs, files in os.walk(INPUT_DIR):\n",
                "    if \"adapter_config.json\" in files:\n",
                "        print(f\"Found adapter candidate at: {root}\")\n",
                "        adapter_candidates.append(root)\n",
                "\n",
                "if not adapter_candidates:\n",
                "    print(\"ERROR: No 'adapter_config.json' found anywhere in input. Did you upload the dataset correctly?\")\n",
                "    print(\"Full directory listing:\")\n",
                "    for root, dirs, files in os.walk(INPUT_DIR):\n",
                "        print(f\"{root} -> {dirs}, {files}\")\n",
                "else:\n",
                "    # Sort candidates to find the latest stage (assuming naming convention or just pick last)\n",
                "    # We prefer paths containing 'Stage4', then 'Stage3', etc.\n",
                "    adapter_candidates.sort(key=lambda x: x, reverse=True)\n",
                "    CHECKPOINT_DIR = adapter_candidates[0]\n",
                "    print(f\"Selected adapter: {CHECKPOINT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Base Model\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "print(\"Loading base model...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Adapter\n",
                "if CHECKPOINT_DIR:\n",
                "    print(f\"Loading adapter from {CHECKPOINT_DIR}...\")\n",
                "    model = FastLanguageModel.get_peft_model(\n",
                "        model,\n",
                "        r = 16,\n",
                "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "        lora_alpha = 16,\n",
                "        lora_dropout = 0,\n",
                "        bias = \"none\",\n",
                "        use_gradient_checkpointing = \"unsloth\",\n",
                "        random_state = 3407,\n",
                "    )\n",
                "    model.load_adapter(CHECKPOINT_DIR)\n",
                "else:\n",
                "    print(\"Skipping adapter load due to missing path.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Export to GGUF\n",
                "if CHECKPOINT_DIR:\n",
                "    print(\"Saving to GGUF (q4_k_m)...\")\n",
                "    try:\n",
                "        model.save_pretrained_gguf(\"model_recovered\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "        \n",
                "        # Move to Output\n",
                "        !mv model_recovered-unsloth.Q4_K_M.gguf /kaggle/working/medical_llama3_kaggle.gguf\n",
                "        print(\"SUCCESS! Model saved to /kaggle/working/medical_llama3_kaggle.gguf\")\n",
                "        print(\"Download it from the 'Output' section on the right sidebar.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Export failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}