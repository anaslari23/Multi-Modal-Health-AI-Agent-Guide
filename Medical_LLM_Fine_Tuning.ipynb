{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Medical LLM Fine-Tuning for Medicine Suggestions\n",
                "\n",
                "This notebook fine-tunes a medical language model using the allopathy medicines dataset to improve medicine suggestion accuracy.\n",
                "\n",
                "**Dataset**: 1,027 medicines with disease conditions, contraindications, side effects, and drug interactions.\n",
                "\n",
                "**Approach**: Fine-tune a small medical LLM (BioGPT or similar) using LoRA/QLoRA for efficient training on Colab's free GPU."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers datasets peft accelerate bitsandbytes wandb trl\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import json\n",
                "from datasets import Dataset, DatasetDict\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    pipeline\n",
                ")\n",
                "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
                "from trl import SFTTrainer\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Upload and Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload your CSV file using Colab's file upload\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Please upload allopathy_medicines_plus_1000.csv\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Load the dataset\n",
                "df = pd.read_csv('allopathy_medicines_plus_1000.csv')\n",
                "print(f\"Loaded {len(df)} medicines\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create instruction-following dataset\n",
                "def create_training_examples(row):\n",
                "    \"\"\"Convert medicine data into instruction-response pairs.\"\"\"\n",
                "    examples = []\n",
                "    \n",
                "    medicine_name = row.get('medicine_name', '') or row.get('generic_name', '')\n",
                "    disease = row.get('disease_condition', '')\n",
                "    category = row.get('therapeutic_category', '')\n",
                "    contraindications = row.get('contraindications', '')\n",
                "    side_effects = row.get('common_side_effects', '')\n",
                "    interactions = row.get('drug_interactions', '')\n",
                "    dosage_form = row.get('dosage_form', '')\n",
                "    \n",
                "    if not medicine_name:\n",
                "        return examples\n",
                "    \n",
                "    # Example 1: Disease to medicine\n",
                "    if disease:\n",
                "        instruction = f\"What medicine can be used for {disease}?\"\n",
                "        response = f\"{medicine_name} is used for {disease}. \"\n",
                "        if category:\n",
                "            response += f\"It belongs to the {category} category. \"\n",
                "        if contraindications:\n",
                "            response += f\"⚠️ Contraindications: {contraindications}. \"\n",
                "        if side_effects:\n",
                "            response += f\"Common side effects: {side_effects}. \"\n",
                "        response += \"Always consult a physician before taking any medication.\"\n",
                "        \n",
                "        examples.append({\n",
                "            'instruction': instruction,\n",
                "            'response': response\n",
                "        })\n",
                "    \n",
                "    # Example 2: Medicine information\n",
                "    instruction = f\"Tell me about {medicine_name}\"\n",
                "    response = f\"{medicine_name} is a medicine\"\n",
                "    if category:\n",
                "        response += f\" in the {category} category\"\n",
                "    if disease:\n",
                "        response += f\" used for {disease}\"\n",
                "    response += \". \"\n",
                "    if dosage_form:\n",
                "        response += f\"Available as {dosage_form}. \"\n",
                "    if contraindications:\n",
                "        response += f\"⚠️ Do not use if: {contraindications}. \"\n",
                "    if side_effects:\n",
                "        response += f\"Side effects: {side_effects}. \"\n",
                "    response += \"Consult a physician for proper dosage.\"\n",
                "    \n",
                "    examples.append({\n",
                "        'instruction': instruction,\n",
                "        'response': response\n",
                "    })\n",
                "    \n",
                "    # Example 3: Contraindications query\n",
                "    if contraindications:\n",
                "        instruction = f\"What are the contraindications for {medicine_name}?\"\n",
                "        response = f\"⚠️ {medicine_name} should NOT be used in the following cases: {contraindications}. Always consult a healthcare professional before taking this medication.\"\n",
                "        \n",
                "        examples.append({\n",
                "            'instruction': instruction,\n",
                "            'response': response\n",
                "        })\n",
                "    \n",
                "    return examples\n",
                "\n",
                "# Generate training examples\n",
                "all_examples = []\n",
                "for _, row in df.iterrows():\n",
                "    all_examples.extend(create_training_examples(row))\n",
                "\n",
                "print(f\"Generated {len(all_examples)} training examples\")\n",
                "\n",
                "# Create dataset\n",
                "train_data = Dataset.from_list(all_examples)\n",
                "print(\"\\nSample example:\")\n",
                "print(train_data[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Format for instruction tuning\n",
                "def format_instruction(example):\n",
                "    \"\"\"Format as instruction-following prompt.\"\"\"\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{example['instruction']}\n",
                "\n",
                "### Response:\n",
                "{example['response']}\"\"\"\n",
                "    return {'text': prompt}\n",
                "\n",
                "train_data = train_data.map(format_instruction)\n",
                "print(\"\\nFormatted example:\")\n",
                "print(train_data[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Base Model with QLoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model selection - using a small medical/general LLM\n",
                "# Options:\n",
                "# - \"microsoft/BioGPT-Large\" - Medical domain\n",
                "# - \"meta-llama/Llama-2-7b-hf\" - General (requires HF token)\n",
                "# - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" - Lightweight\n",
                "\n",
                "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Fast training on free Colab\n",
                "\n",
                "# QLoRA configuration for 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(f\"Model loaded: {model_name}\")\n",
                "print(f\"Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configure LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA configuration\n",
                "peft_config = LoraConfig(\n",
                "    r=16,  # LoRA rank\n",
                "    lora_alpha=32,  # LoRA alpha\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Attention layers\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./medical-medicine-lora\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    fp16=True,\n",
                "    save_total_limit=2,\n",
                "    logging_steps=10,\n",
                "    save_steps=100,\n",
                "    warmup_steps=50,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    report_to=\"none\",  # Change to \"wandb\" if you want tracking\n",
                ")\n",
                "\n",
                "# SFT Trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=train_data,\n",
                "    peft_config=peft_config,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=512,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "print(\"Trainer configured successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start training\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save LoRA adapters\n",
                "model.save_pretrained(\"medical-medicine-lora-final\")\n",
                "tokenizer.save_pretrained(\"medical-medicine-lora-final\")\n",
                "\n",
                "print(\"Model saved to: medical-medicine-lora-final/\")\n",
                "\n",
                "# Download to local machine\n",
                "!zip -r medical-medicine-lora-final.zip medical-medicine-lora-final/\n",
                "files.download('medical-medicine-lora-final.zip')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test the Fine-Tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test inference\n",
                "def generate_response(instruction):\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{instruction}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        do_sample=True\n",
                "    )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract only the response part\n",
                "    response = response.split(\"### Response:\")[-1].strip()\n",
                "    return response\n",
                "\n",
                "# Test queries\n",
                "test_queries = [\n",
                "    \"What medicine can be used for fever?\",\n",
                "    \"Tell me about Paracetamol\",\n",
                "    \"What are the contraindications for Ibuprofen?\",\n",
                "    \"Suggest medicine for bacterial infection\"\n",
                "]\n",
                "\n",
                "print(\"Testing fine-tuned model:\\n\")\n",
                "for query in test_queries:\n",
                "    print(f\"Q: {query}\")\n",
                "    print(f\"A: {generate_response(query)}\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Convert to GGUF for Local Use (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install llama.cpp converter\n",
                "!pip install -q llama-cpp-python\n",
                "\n",
                "# Merge LoRA with base model\n",
                "from peft import PeftModel\n",
                "\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "merged_model = PeftModel.from_pretrained(base_model, \"medical-medicine-lora-final\")\n",
                "merged_model = merged_model.merge_and_unload()\n",
                "\n",
                "# Save merged model\n",
                "merged_model.save_pretrained(\"medical-medicine-merged\")\n",
                "tokenizer.save_pretrained(\"medical-medicine-merged\")\n",
                "\n",
                "print(\"Merged model saved!\")\n",
                "\n",
                "# Convert to GGUF (requires llama.cpp)\n",
                "# !python llama.cpp/convert.py medical-medicine-merged --outtype q4_k_m --outfile medical-medicine.gguf"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Integration Instructions\n",
                "\n",
                "### To use this model in your project:\n",
                "\n",
                "1. **Download the model** from this notebook\n",
                "2. **Upload to your backend** at `mm-hie-backend/models/medical-medicine-lora/`\n",
                "3. **Update RAG engine** to use the fine-tuned model:\n",
                "\n",
                "```python\n",
                "# In app/rag/rag_engine.py\n",
                "from peft import PeftModel\n",
                "\n",
                "# Load fine-tuned model\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model = PeftModel.from_pretrained(base_model, \"./models/medical-medicine-lora\")\n",
                "```\n",
                "\n",
                "### Benefits:\n",
                "- ✅ **Accurate medicine suggestions** based on diseases\n",
                "- ✅ **Contraindication awareness** - knows when NOT to suggest medicines\n",
                "- ✅ **Side effects knowledge** - can warn about common side effects\n",
                "- ✅ **Drug interaction awareness** - understands medicine interactions\n",
                "- ✅ **Small model size** (~1-2GB) - runs on CPU/GPU efficiently"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}