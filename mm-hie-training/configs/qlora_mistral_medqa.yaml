model_name: "mistralai/Mistral-7B-Instruct-v0.2"
output_dir: "checkpoints/mistral-medical-qlora"
learning_rate: 1e-4
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 8
warmup_steps: 100
max_seq_length: 1024
logging_steps: 20
save_steps: 500
evaluation_strategy: "epoch"
bf16: true
fp16: false
bits: 4
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: "nf4"
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
