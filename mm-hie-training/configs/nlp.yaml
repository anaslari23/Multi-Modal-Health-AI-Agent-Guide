train:
  epochs: 6
  batch_size: 16
  lr: 2e-5
  weight_decay: 0.01
  warmup_steps: 500

data:
  train_path: data/nlp/train.jsonl
  val_path: data/nlp/val.jsonl
  max_length: 256

model:
  pretrained: emilyalsentzer/Bio_ClinicalBERT
  num_labels: 50

logging:
  wandb_project: mmhie-nlp
  save_every: 1
  checkpoint_dir: checkpoints/nlp
